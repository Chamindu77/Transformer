# Attention Mechanism â€“ A Gentle Introduction

## ğŸŒŸ What Youâ€™ll Learn

- How words are represented as vectors (embeddings)
- The role of **Queries (Q)**, **Keys (K)**, and **Values (V)**
- How dot-product attention is computed step by step
- Why scaling and softmax are applied
- The final attention-weighted output

## ğŸ“ Step-by-Step Summary

1. **Word Embeddings** â€“ Convert words ("The", "cat", "sat") into numeric vectors.
2. **Q, K, V Matrices** â€“ Apply linear transformations to get Queries, Keys, and Values.
3. **Dot Products** â€“ Compare Queries with Keys to measure relevance.
4. **Scaling** â€“ Normalize scores by dividing by âˆšdâ‚– to prevent large values.
5. **Softmax** â€“ Turn scores into probabilities (attention weights).
6. **Weighted Sum** â€“ Multiply weights by Values â†’ this gives the context-aware representation.

## ğŸ’» Code Highlight

The notebook demonstrates attention in just a few lines of NumPy:

```python
scores = Q @ K.T              # similarity
scaled = scores / np.sqrt(dk) # scaling
weights = softmax(scaled)     # attention weights
output = weights @ V          # final output
```

## ğŸš€ Key Takeaways

- Attention tells the model *where to look* in a sequence.
- Queries ask a question, Keys provide possible matches, Values supply the actual information.
- This mechanism is the foundation of **Transformers, BERT, and GPT**.

---

âœ¨ Explore the notebook in this repo to see the math come alive with code!

